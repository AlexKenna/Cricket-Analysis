{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "...\n",
    "\n",
    "To begin with, we will perform the necessary imports and load the summary dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and information.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from lib.constants import DATA_PATH\n",
    "\n",
    "# Load batter summary data.\n",
    "summary = pd.read_csv(\n",
    "    DATA_PATH + \"/Batter_Summary_Reduced.txt\", delimiter=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the Data.\n",
    "Before tuning hyperparameters for the Random Forest model, we must first prepare the data. This includes:\n",
    "\n",
    "1. Removing unnecessary fields (e.g., Name).\n",
    "2. Fill in missing data.\n",
    "3. Split the data into test and taining sets.\n",
    "\n",
    "This process is performed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace batter hand with a numeric representation (Right Hand = 0, Left Hand = 1).\n",
    "summary = summary.drop(columns=[\"Name\", \"Batter_ID\"])\n",
    "\n",
    "# Fill missing data with the columns mean.\n",
    "data = summary.copy()\n",
    "for i in data.columns[data.isnull().any(axis=0)]:\n",
    "    data[i].fillna(data[i].median(), inplace=True)\n",
    "\n",
    "# Extract features and labels.\n",
    "X = data.drop(columns=[\"International_One_Day_Batting_Average\"])\n",
    "y = data[\"International_One_Day_Batting_Average\"]\n",
    "\n",
    "# Split the training and test datasets.\n",
    "X_train = X[:-12]\n",
    "X_test = X[-12:]\n",
    "y_train = y[:-12]\n",
    "y_test = y[-12:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Hyperparameter Search.\n",
    "While it is possible to speculate the best hyperparameters for the Random Forest based on theoretical findings in literature, it is often more efficient to try a wide range of values to see what combination of hyperparameters works best. However, once a set of optimised hyperparameters have been chosen, we will explore their significance from a theoretical perspective.\n",
    "\n",
    "The following hyperparameters will be tuned:\n",
    "\n",
    "* n_estimators\n",
    "* max_features\n",
    "* max_depth\n",
    "* min_samples_split\n",
    "* min_samples_leaf\n",
    "* bootstrap\n",
    "\n",
    "**n_estimators**<br>\n",
    "Defines the number of trees in the Random Forest.\n",
    "\n",
    "A random forest is an extension of bootstrap aggregation of decision trees, making it an ensemble of decision trees. The n_estimators hyperparameter defines how many decision trees the random forest will contain. Typically, a higher number of trees will result in a better learning of the data, however, at the expense of a longer training process. At some point, increasing the number of trees will result in diminishing returns from the model.\n",
    "\n",
    "To stabilise the error rate of the random forest, it is generally recommended to begin with ten times as many trees as there are features. However, this number should be raised or lowered depending on the other hyperparameters selected.\n",
    "\n",
    "**max_features**<br>\n",
    "Defines the number of features to consider at each split.\n",
    "\n",
    "At each split, a certain number of features (max_features) are randomly selected from the dataset. From these randomly selected features, one is chosen as the best for splitting the node. This parameter reduces overfitting and increases the stability of the trees.\n",
    "\n",
    "Depending on the computational cost and overfitting present in the model, it is typical to use fewer features (log2) or more features (sqrt) as necessary. It is also possible to provide a custom float for further fine-tuning.\n",
    "\n",
    "> max_features is calculated as: sqrt(n_features), log_2(n_features), etc.\n",
    "\n",
    "**max_depth**<br>\n",
    "Defines the number of levels in the tree.\n",
    "\n",
    "Theoretically, the maximum depth of a decision tree is one less than the number of samples, however, overfitting will occur before this is achieved. This occurs as the deeper a tree grows, the more complex it becomes and will capture more information about the dataset. Once this occurs, you must reduce the maximum depth. However, if the depth is too shallow, underfitting will occur.\n",
    "\n",
    "There is no single value typically recommended for max_depth. Generally, the approach is to experimentally choose a value that does not overfit or underfit the data.\n",
    "\n",
    "**min_samples_split**<br>\n",
    "Defines the minimum number of samples required to split a node.\n",
    "\n",
    "A node, not to be confused with a leaf node, is a node with children (also known as an internal node). If an internal node has fewer samples than min_samples_split, then the node is not permitted to split. For example, if min_samples_split = 7 and a node only contains 5 samples, it will not split. This parameter is intended to control overfitting. Higher values prevent the model from learning relations specific only to the sample it was provided, however, too high a value can result in underfitting.\n",
    "\n",
    "Typically, ideal values range between 1 and 40 for CART algorithm, which is used in this project.\n",
    "\n",
    "**min_samples_leaf**<br>\n",
    "Defines the minimum number of samples required at each leaf node.\n",
    "\n",
    "A leaf node is a node without children. If splitting an internal node results in a leaf with fewer samples than min_samples_leaf, the split will not be permitted. For example, if min_samples_leaf = 2 and splitting an internal node results in a leaf node with 1 sample, the split will not occur. \n",
    "\n",
    "Typically, ideal values range between 1 and 20 for the CART algorithm.\n",
    "\n",
    "**bootstrap**<br>\n",
    "Defines whether bootstrapping should be used to select samples for training each tree.\n",
    "\n",
    "Bootstrapping is a resampling technique used to create a random subset of data for each tree by using random sampling with replacement. This results in approximately one-third of instances being left out of each tree. The idea is that although each tree might have high variance for a particular set of the training data, overall, the entire forest will have a lower variance. If bootstrapping is not used, the same training set is used for each tree in the forest and overall variance would be expected to be greater.\n",
    "\n",
    "It is generally recommended that bootstrapping be used to reduce the variance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create Random Hyperparameter Grid.\n",
    "To perform a random hyperparameter search, we first need to create a grid containing the possible values for each parameter to sample from.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in the forest.\n",
    "n_estimators = [int(x) for x in np.linspace(start=100, stop=500, num=21)]\n",
    "\n",
    "# Number of features to consider at every split.\n",
    "max_features = [\"auto\", \"sqrt\"]\n",
    "\n",
    "# Maximum number of levels in each tree.\n",
    "max_depth = [int(x) for x in np.linspace(start=10, stop=40, num=11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node.\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node.\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree.\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the parameter grid.\n",
    "param_grid = {\"n_estimators\": n_estimators,\n",
    "              \"max_features\": max_features,\n",
    "              \"max_depth\": max_depth,\n",
    "              \"min_samples_split\": min_samples_split,\n",
    "              \"min_samples_leaf\": min_samples_leaf,\n",
    "              \"bootstrap\": bootstrap}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Randomised Search.\n",
    "With the random parameter grid defined, we now wish to test combinations of hyperparameters. If we were to test all possible combinations from the random parameter grid defined previously, we would have 9072 tests to perform. Instead, RandomizedSearchCV is used to narrow down the possible values of the optimal hyperparameters. This method will allow a random selection of combinations to be tested, reducing the time taken for testing at the expense of a less thorough search. However, as this is only being used to narrow down the possible parameters, this trade-off is acceptable.\n",
    "\n",
    "Below, we run RandomizedSearchCV to test 2000 combinations, performing three-fold cross-validation for each combination. From this, we can extract the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 160,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 22,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the random forest regressor.\n",
    "rfr = RandomForestRegressor()\n",
    "\n",
    "# Create the random search cross-validator.\n",
    "rfr_random = RandomizedSearchCV(\n",
    "    estimator=rfr, param_distributions=param_grid, n_iter=2000, cv=3, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model.\n",
    "rfr_random.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters for the model.\n",
    "rfr_random.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Test Accuracy.\n",
    "With the reduced hyperparameters selected, we can test the accuracy of the refined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model using Random Search hyperparameters is: 0.7771169381040898\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "num_tests = 250\n",
    "\n",
    "for _ in range(num_tests):\n",
    "  # Create the random forest regressor.\n",
    "  rfr = RandomForestRegressor(n_estimators=160, min_samples_split=5,\n",
    "                              min_samples_leaf=4, max_features=\"sqrt\", max_depth=22, bootstrap=True)\n",
    "\n",
    "  # Fit the model.\n",
    "  rfr.fit(X_train, y_train)\n",
    "\n",
    "  # Test the accuracy of the model.\n",
    "  accuracy += rfr.score(X_test, y_test)\n",
    "\n",
    "print(\"The accuracy of the model using Random Search hyperparameters is: {}\".format(\n",
    "    accuracy / num_tests))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Refine Hyperparameters.\n",
    "Random search allowed us to narrow down the optimal values for the hyperparameters. Now, we can perform a more thorough search around the refined parameters using Grid Search. This method will test all possible combinations of hyperparameter values we provide it, and choose the best combination for the Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create Refined Parameter Grid.\n",
    "To perform a grid search, we first need to create the grid of possible values for each parameter to test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid.\n",
    "param_grid = {\"n_estimators\": [int(x) for x in np.linspace(140, 160, 11)],\n",
    "              \"max_features\": [2, 3, 4, 5, 6],\n",
    "              \"max_depth\": [None] + [int(x) for x in np.linspace(15, 25, 11)],\n",
    "              \"min_samples_split\": [2, 3, 4],\n",
    "              \"min_samples_leaf\": [1, 2, 3],\n",
    "              \"bootstrap\": [True]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Grid Search.\n",
    "From the parameter grid, we can perform a grid search to test all combinations of the possible hyperparameter values using five-fold cross-validation. From this, we can determine the most optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 16,\n",
       " 'max_features': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 3,\n",
       " 'n_estimators': 142}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the random forest regressor.\n",
    "rfr = RandomForestRegressor()\n",
    "\n",
    "# Create the random search cross-validator.\n",
    "rfr_random = GridSearchCV(\n",
    "    estimator=rfr, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model.\n",
    "rfr_random.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters for the model.\n",
    "rfr_random.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Test Accuracy.\n",
    "With the optimal hyperparameters selected, we can now test the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model using Grid Search hyperparameters is: 0.7855894094824831\n"
     ]
    }
   ],
   "source": [
    "# Initialise variabled for determining average accuracy.\n",
    "accuracy = 0\n",
    "num_tests = 250\n",
    "\n",
    "for _ in range(num_tests):\n",
    "  # Create the random forest regressor.\n",
    "  rfr = RandomForestRegressor(n_estimators=142, min_samples_split=3,\n",
    "                              min_samples_leaf=1, max_features=2, max_depth=16, bootstrap=True)\n",
    "\n",
    "  # Fit the model.\n",
    "  rfr.fit(X_train, y_train)\n",
    "\n",
    "  # Test the accuracy of the model.\n",
    "  accuracy += rfr.score(X_test, y_test)\n",
    "\n",
    "# Print the accuracy.\n",
    "print(\"The accuracy of the model using Grid Search hyperparameters is: {}\".format(\n",
    "    accuracy / num_tests))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5f05966c834b7114444b779d720abe0e299bd07f35b3e38633408fa0bc32ead"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
